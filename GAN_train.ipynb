{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import data\n",
    "import models, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.data_path= 'data'\n",
    "        self.datasetG= 'masked_pwc'\n",
    "        self.datasetD= 'pwc'\n",
    "        self.batch_size= 128\n",
    "        self.model= 'unet1d'\n",
    "        self.modelG= 'unet1d'\n",
    "        self.modelD= 'gan_discriminator'\n",
    "        self.lr= 0.001\n",
    "        self.num_epochs= 10\n",
    "        self.n_data = 1000\n",
    "        self.min_sep = 5\n",
    "        self.valid_interval= 2\n",
    "        self.save_interval= 2\n",
    "        self.seed = 0\n",
    "        self.output_dir= 'experiments'\n",
    "        self.experiment= None\n",
    "\n",
    "        self.resume_training= False\n",
    "        self.restore_file= None\n",
    "\n",
    "        self.no_save= False\n",
    "        self.step_checkpoints= False\n",
    "        self.no_log= False\n",
    "        self.log_interval= 100\n",
    "        self.no_visual= False\n",
    "        self.visual_interval= 100\n",
    "        self.no_progress= False\n",
    "        self.draft= False\n",
    "        self.dry_run= False\n",
    "        self.g_d_update_ratio = 2\n",
    "        self.in_channels= 1\n",
    "        self.hidden_size = 64\n",
    "        self.bias= False\n",
    "        self.test_num = 0\n",
    "        self.wtl2 = 0 # l2 loss weighting\n",
    "        # UNET\n",
    "        self.residual = False\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-02 22:39:09] COMMAND: /home/michael/python-virtual-environments/bfcnn/lib/python3.6/site-packages/ipykernel_launcher.py -f /home/michael/.local/share/jupyter/runtime/kernel-fdd9f728-7605-48d5-b105-fcd99f8977ee.json\n",
      "[2020-09-02 22:39:09] Arguments: {'data_path': 'data', 'datasetG': 'masked_pwc', 'datasetD': 'pwc', 'batch_size': 128, 'model': 'unet1d', 'modelG': 'unet1d', 'modelD': 'gan_discriminator', 'lr': 0.001, 'num_epochs': 10, 'n_data': 1000, 'min_sep': 5, 'valid_interval': 2, 'save_interval': 2, 'seed': 0, 'output_dir': 'experiments', 'experiment': 'unet1d-Sep-02-22:39:09', 'resume_training': False, 'restore_file': None, 'no_save': False, 'step_checkpoints': False, 'no_log': False, 'log_interval': 100, 'no_visual': False, 'visual_interval': 100, 'no_progress': False, 'draft': False, 'dry_run': False, 'g_d_update_ratio': 2, 'in_channels': 1, 'hidden_size': 64, 'bias': False, 'test_num': 0, 'wtl2': 0, 'residual': False, 'experiment_dir': 'experiments/unet1d/unet1d-Sep-02-22:39:09', 'checkpoint_dir': 'experiments/unet1d/unet1d-Sep-02-22:39:09/checkpoints', 'log_dir': 'experiments/unet1d/unet1d-Sep-02-22:39:09/logs', 'log_file': 'experiments/unet1d/unet1d-Sep-02-22:39:09/logs/train.log'}\n"
     ]
    }
   ],
   "source": [
    "# gpu or cpu\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "utils.setup_experiment(args)\n",
    "utils.init_logging(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# IF NOT RETRAINING\n",
    "# Build data loaders, a model and an optimizer\n",
    "\n",
    "G,D = models.build_model_gan(args)\n",
    "netG = G.to(device)\n",
    "netD = D.to(device)\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# 2 optimizers\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=args.lr,betas=(0.5, 0.999))\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=args.lr,betas=(0.5, 0.999))\n",
    "\n",
    "logging.info(f\"Built a generator model consisting of {sum(p.numel() for p in netG.parameters()):,} parameters\")\n",
    "logging.info(f\"Built a discriminator model consisting of {sum(p.numel() for p in netD.parameters()):,} parameters\")\n",
    "\n",
    "if args.resume_training and args.restore_file is not None:\n",
    "    state_dict = utils.load_checkpoint_GAN(args, netG, netD, optimizerG, optimizerD)\n",
    "    global_step = state_dict['last_step']\n",
    "    start_epoch = int(state_dict['last_step']/(403200/state_dict['args'].batch_size))+1\n",
    "else:\n",
    "    global_step = -1\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-02 22:39:14] Built a generator model consisting of 72,000 parameters\n",
      "[2020-09-02 22:39:14] Built a discriminator model consisting of 692,224 parameters\n"
     ]
    }
   ],
   "source": [
    "# IF RETRAINING\n",
    "G,D = models.build_model_gan(args)\n",
    "netG = G.to(device)\n",
    "netD = D.to(device)\n",
    "\n",
    "G_PATH = \"models/trained/GAN_unet_generator_100kdata_300epoch_08_20_20.pth\"\n",
    "D_PATH = \"models/trained/GAN_discriminator_100kdata_300epoch_08_20_20.pth\"\n",
    "netG.load_state_dict(torch.load(G_PATH))\n",
    "netD.load_state_dict(torch.load(D_PATH))\n",
    "\n",
    "# 2 optimizers\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=args.lr,betas=(0.5, 0.999))\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=args.lr,betas=(0.5, 0.999))\n",
    "\n",
    "logging.info(f\"Built a generator model consisting of {sum(p.numel() for p in netG.parameters()):,} parameters\")\n",
    "logging.info(f\"Built a discriminator model consisting of {sum(p.numel() for p in netD.parameters()):,} parameters\")\n",
    "\n",
    "global_step = -1\n",
    "start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_dataset is a function in utils/data/__init__.py\n",
    "train_loaderG, valid_loaderG, _ = data.build_dataset(args.datasetG,\n",
    "                                                   args.n_data, \n",
    "                                                   batch_size=args.batch_size,\n",
    "                                                   min_sep = args.min_sep)\n",
    "train_loaderD, _, _ = data.build_dataset(args.datasetD,\n",
    "                                                   args.n_data, \n",
    "                                                   batch_size=args.batch_size,\n",
    "                                                   min_sep = args.min_sep)\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track moving average of loss values\n",
    "train_meters = {name: utils.RunningAverageMeter(0.98) for name in ([\"train_loss\", \"train_psnr\", \"train_ssim\"])}\n",
    "valid_meters = {name: utils.AverageMeter() for name in ([\"valid_psnr\", \"valid_ssim\"])}\n",
    "writer = SummaryWriter(log_dir=args.experiment_dir) if not args.no_visual else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 00:   0%|          | 0/8 [00:00<?, ?it/s, train_loss=8.612 (8.612), train_psnr=28.528 (28.528), train_ssim=0.904 (0.904), lr=1.0e-03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10][0/8]\tLoss_D: 0.0000\tLoss_G: 8.6125\tD(x): 1.0000\tD(G(z)): 0.0000 / 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-02 22:39:32] epoch 00 | train_loss 8.044 | train_psnr 28.597 | train_ssim 0.904 | valid_psnr 28.995 | valid_ssim 0.906 | lr 1.0e-03       \n",
      "epoch 01:  12%|█▎        | 1/8 [00:00<00:02,  2.83it/s, train_loss=6.495 (4.179), train_psnr=29.021 (27.936), train_ssim=0.907 (0.895), lr=1.0e-03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10][0/8]\tLoss_D: 0.0319\tLoss_G: 6.5421\tD(x): 0.9703\tD(G(z)): 0.0012 / 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 02:  12%|█▎        | 1/8 [00:00<00:02,  2.59it/s, train_loss=7.067 (3.980), train_psnr=29.129 (29.363), train_ssim=0.900 (0.900), lr=1.0e-03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/10][0/8]\tLoss_D: 0.0015\tLoss_G: 7.1305\tD(x): 0.9999\tD(G(z)): 0.0014 / 0.0011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-09-02 22:39:38] epoch 02 | train_loss 7.118 | train_psnr 29.037 | train_ssim 0.900 | valid_psnr 28.903 | valid_ssim 0.902 | lr 1.0e-03        \n",
      "epoch 03:  12%|█▎        | 1/8 [00:00<00:02,  2.72it/s, train_loss=0.759 (0.332), train_psnr=29.364 (28.049), train_ssim=0.905 (0.898), lr=1.0e-03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3/10][0/8]\tLoss_D: 7.6039\tLoss_G: 0.7678\tD(x): 0.0008\tD(G(z)): 0.0001 / 0.5179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 04:  12%|█▎        | 1/8 [00:00<00:02,  2.68it/s, train_loss=0.104 (0.336), train_psnr=28.120 (28.255), train_ssim=0.901 (0.907), lr=1.0e-03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4/10][0/8]\tLoss_D: 1.7201\tLoss_G: 0.0988\tD(x): 0.2844\tD(G(z)): 0.2724 / 0.9093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-abd22c248e9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mvalid_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loaderG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msample_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-virtual-environments/bfcnn/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-virtual-environments/bfcnn/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-virtual-environments/bfcnn/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-virtual-environments/bfcnn/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-virtual-environments/bfcnn/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python-virtual-environments/bfcnn/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    292\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecv_handle\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;34m'''Receive a handle over a local connection.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_UNIX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecvfds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDupFd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecvfds\u001b[0;34m(sock, size)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mbytes_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecvmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMSG_SPACE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fake_label = 0.\n",
    "real_label = 1.\n",
    "# fake_label = [0.,0.1]\n",
    "# real_label = [0.9,1.]\n",
    "\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "if args.wtl2 > 0:\n",
    "    use_mse = True\n",
    "else:\n",
    "    use_mse = False\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "for epoch in range(start_epoch, args.num_epochs):\n",
    "    if args.resume_training:\n",
    "        if epoch %10 == 0:\n",
    "            optimizerG.param_groups[0][\"lr\"] /= 2\n",
    "            print('learning rate reduced by factor of 2')\n",
    "\n",
    "    train_bar = utils.ProgressBar(train_loaderG, epoch)\n",
    "    for meter in train_meters.values():\n",
    "        meter.reset()\n",
    "\n",
    "    for batch_id, ((clean, mask),real) in enumerate(zip(train_bar,train_loaderD)):\n",
    "        \n",
    "        ###############################\n",
    "        # First train the discriminator\n",
    "        ###############################\n",
    "        # Only update discriminator based on update ratio\n",
    "        real_cpu = real.to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,),real_label,device=device)\n",
    "\n",
    "        if batch_id % args.g_d_update_ratio == 0:\n",
    "            netD.zero_grad()\n",
    "\n",
    "            # Forward pass real batch through D\n",
    "            output = netD(real_cpu).view(-1)\n",
    "            # Calculate loss on all-real batch\n",
    "            errD_real = criterion(output, label)\n",
    "            # Calculate gradients for D in backward pass\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "        \n",
    "        ## Train with all-fake batch\n",
    "        # Generate fake signal batch with G\n",
    "        inputs = clean.to(device)\n",
    "        mask_inputs = mask.to(device)\n",
    "        # only use the mask part of the outputs\n",
    "        raw_outputs = netG(inputs,mask_inputs)\n",
    "        fake = (1-mask_inputs)*raw_outputs + mask_inputs*inputs\n",
    "        \n",
    "        label.fill_(fake_label)\n",
    "        if batch_id % args.g_d_update_ratio == 0:\n",
    "            # Classify all fake batch with D\n",
    "            output = netD(fake.detach()).view(-1)\n",
    "            # Calculate D's loss on the all-fake batch\n",
    "            errD_fake = criterion(output, label)\n",
    "            # Calculate the gradients for this batch\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            # Add the gradients from the all-real and all-fake batches\n",
    "            errD = errD_real + errD_fake\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "        \n",
    "        ###############################\n",
    "        # Next, train the generator\n",
    "        ###############################\n",
    "        netG.zero_grad() # train() or zero_grad()?\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        \n",
    "        if use_mse:\n",
    "            # Calculate G's loss based on this output\n",
    "            errG_D = criterion(output, label)\n",
    "            # MSE Loss\n",
    "            errG_l2 = F.mse_loss(fake, inputs, reduction=\"sum\") / (inputs.size(0) * 2)\n",
    "            errG =  (1-args.wtl2) * errG_D + args.wtl2 * errG_l2\n",
    "        else:\n",
    "            errG = criterion(output, label)\n",
    "        \n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        \n",
    "\n",
    "        \n",
    "        # Output training stats\n",
    "        if batch_id % 50 == 0:\n",
    "            if use_mse:\n",
    "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f + %.4f = %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, args.num_epochs, batch_id, len(train_loaderG), \\\n",
    "                     errD.item(), errG_D.item(), errG_l2, errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            else: \n",
    "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, args.num_epochs, batch_id, len(train_loaderG),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        global_step += 1\n",
    "        \n",
    "        # TO DO, only run loss on masked part of output\n",
    "        # TO DO, incorporate MSE loss into GAN?\n",
    "        # loss = F.mse_loss(outputs, inputs, reduction=\"sum\") / (inputs.size(0) * 2)\n",
    "\n",
    "        train_psnr = utils.psnr(fake, inputs)\n",
    "        train_ssim = utils.ssim(fake, inputs)\n",
    "        train_meters[\"train_loss\"].update(errG.item())\n",
    "        train_meters[\"train_psnr\"].update(train_psnr.item())\n",
    "        train_meters[\"train_ssim\"].update(train_ssim.item())\n",
    "        train_bar.log(dict(**train_meters, lr=optimizerG.param_groups[0][\"lr\"]), verbose=True)\n",
    "\n",
    "        if writer is not None and global_step % args.log_interval == 0:\n",
    "            writer.add_scalar(\"lr\", optimizerG.param_groups[0][\"lr\"], global_step)\n",
    "            writer.add_scalar(\"loss/train\", errG.item(), global_step)\n",
    "            writer.add_scalar(\"psnr/train\", train_psnr.item(), global_step)\n",
    "            writer.add_scalar(\"ssim/train\", train_ssim.item(), global_step)\n",
    "            gradients = torch.cat([p.grad.view(-1) for p in netG.parameters() if p.grad is not None], dim=0)\n",
    "            writer.add_histogram(\"gradients\", gradients, global_step)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    if epoch % args.valid_interval == 0:\n",
    "        netG.eval()\n",
    "        for meter in valid_meters.values():\n",
    "            meter.reset()\n",
    "\n",
    "        valid_bar = utils.ProgressBar(valid_loaderG)\n",
    "        \n",
    "        for sample_id, (clean, mask) in enumerate(valid_bar):\n",
    "            with torch.no_grad():\n",
    "                inputs = clean.to(device)\n",
    "                mask_inputs = mask.to(device)\n",
    "                # only use the mask part of the outputs\n",
    "                raw_output = netG(inputs,mask_inputs)\n",
    "                output = (1-mask_inputs)*raw_output + mask_inputs*inputs\n",
    "                valid_psnr = utils.psnr(inputs, output)\n",
    "                valid_meters[\"valid_psnr\"].update(valid_psnr.item())\n",
    "                valid_ssim = utils.ssim(inputs, output)\n",
    "                valid_meters[\"valid_ssim\"].update(valid_ssim.item())\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"psnr/valid\", valid_meters['valid_psnr'].avg, global_step)\n",
    "            writer.add_scalar(\"ssim/valid\", valid_meters['valid_ssim'].avg, global_step)\n",
    "            sys.stdout.flush()\n",
    "            utils.save_checkpoint_GAN(args, global_step, netG, netD, optimizerG, optimizerD, score=valid_meters[\"valid_psnr\"].avg, mode=\"max\")\n",
    "\n",
    "        logging.info(train_bar.print(dict(**train_meters, **valid_meters, lr=optimizerG.param_groups[0][\"lr\"])))\n",
    "        # Save the loss curve plot\n",
    "utils.save_losses_curve(G_losses,D_losses,args)\n",
    "# logging.info(f\"Done training! Best PSNR {utils.save_checkpoint_GAN.best_score:.3f} obtained after step {utils.save_checkpoint.best_step}.\")\n",
    "logging.info(f\"Done training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Best PSNR 28.560\n",
    "def mask_idx_f(mask):\n",
    "    mask_start = int(np.argmin(mask[0]))\n",
    "    mask_length = int((1-mask[0]).sum())\n",
    "    mask_idx = range(mask_start,mask_start+mask_length)\n",
    "     # No mask indices\n",
    "    before = np.arange(mask.shape[2])[:mask_start]\n",
    "    after = np.arange(mask.shape[2])[mask_start+mask_length:]\n",
    "    no_mask_idx = np.append(before,after)\n",
    "    return mask_idx,before, after, mask_length, mask_start\n",
    "\n",
    "def print_one(loader,model1,model2):\n",
    "    np.random.seed()\n",
    "    clean,mask = next(iter(loader))\n",
    "    outputs1 = model1(clean.to(device),mask.to(device)).cpu()\n",
    "    outputs2 = model2(clean.to(device),mask.to(device)).cpu()\n",
    "    \n",
    "    mask_idx,before_mask,after_mask,mask_length, mask_start = mask_idx_f(mask)\n",
    "\n",
    "    out1 = outputs1[0] * (1-mask[0]) + clean[0]*mask[0]\n",
    "    out2 = outputs2[0] * (1-mask[0]) + clean[0]*mask[0]\n",
    "\n",
    "    print(\"Mask Length: {}\\tMask Start: {}\".format(mask_length,mask_start))\n",
    "    \n",
    "    plt.figure(figsize=[15,14])\n",
    "    plt.subplot(4,1,1)\n",
    "    plt.plot(clean[0,0,:],'xb')\n",
    "    plt.plot(mask_idx,np.zeros(len(mask_idx)),'--k')\n",
    "    plt.plot(mask_idx,np.ones(len(mask_idx)),'--k')\n",
    "    plt.title(\"True signal\")\n",
    "\n",
    "    plt.subplot(4,1,2)\n",
    "    masked = clean[0]*mask[0]\n",
    "    masked_plot = masked[:mask_start,]\n",
    "    plt.plot(before_mask,masked[0,before_mask],'xb')\n",
    "    plt.plot(after_mask,masked[0,after_mask],'xb')\n",
    "    plt.plot(mask_idx,np.zeros(len(mask_idx)),'--k')\n",
    "    plt.plot(mask_idx,np.ones(len(mask_idx)),'--k')\n",
    "    plt.title(\"Masked signal\")\n",
    "\n",
    "    plt.subplot(4,1,3)\n",
    "    plt.plot(out1[0,:].detach(),'xb')\n",
    "    plt.plot(mask_idx,np.zeros(len(mask_idx)),'--k')\n",
    "    plt.plot(mask_idx,np.ones(len(mask_idx)),'--k')\n",
    "    plt.title(\"GAN denoised signal\")\n",
    "    print(outputs1[0,:].detach())\n",
    "    \n",
    "    plt.subplot(4,1,4)\n",
    "    plt.plot(out2[0,:].detach(),'xb')\n",
    "    plt.plot(mask_idx,np.zeros(len(mask_idx)),'--k')\n",
    "    plt.plot(mask_idx,np.ones(len(mask_idx)),'--k')\n",
    "    plt.title(\"MSE denoised signal\")\n",
    "#     return out1,out2, clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MSE model\n",
    "mse = models.build_model(args)\n",
    "netMSE = mse.to(device)\n",
    "\n",
    "MODEL_PATH = \"models/trained/unet1d_partialconv_10kdata_30epoch_3minsep_08_14_20.pth\"\n",
    "netMSE.load_state_dict(torch.load(MODEL_PATH))\n",
    "netMSE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loader is shuffled and allows test_num to force a certain mask shape\n",
    "_, _, test_loader = data.build_dataset(args.datasetG,\n",
    "                                                   1000, \n",
    "                                                   batch_size=1,\n",
    "                                                   fix_datapoints=False,\n",
    "                                                   min_sep = args.min_sep,\n",
    "                                                   test_num = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_one(test_loader,netG,netMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_one(test_loader,netG,netMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_one(test_loader,netG,netMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_one(test_loader,netG,netMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_one(test_loader,netG,netMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_one(test_loader,netG,netMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the model (Old way)\n",
    "MODEL_PATHG = \"models/trained/GAN_unet_generator_100kdata_300epoch_08_20_20.pth\"\n",
    "MODEL_PATHD = \"models/trained/GAN_discriminator_100kdata_300epoch_08_20_20.pth\"\n",
    "torch.save(netG.state_dict(), MODEL_PATHG)\n",
    "torch.save(netD.state_dict(), MODEL_PATHD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Introducing label noise\n",
    "#         label = torch.rand((b_size,),device=device)*(fake_label[1]-fake_label[0])+fake_label[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
