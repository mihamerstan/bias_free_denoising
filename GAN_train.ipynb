{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import data\n",
    "import models, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.data_path= 'data'\n",
    "        self.datasetG= 'masked_pwc'\n",
    "        self.datasetD= 'pwc'\n",
    "        self.batch_size= 128\n",
    "        self.model= 'unet1d'\n",
    "        self.modelG= 'unet1d'\n",
    "        self.modelD= 'gan_discriminator'\n",
    "        self.lr= 0.001\n",
    "        self.num_epochs= 25\n",
    "        self.n_data = 500\n",
    "        self.min_sep = 5\n",
    "        self.valid_interval= 1\n",
    "        self.save_interval= 1\n",
    "        self.seed = 0\n",
    "        self.output_dir= 'experiments'\n",
    "        self.experiment= None\n",
    "        self.resume_training= True\n",
    "        self.restore_file= \"experiments/unet1d/unet1d-Aug-30-14:00:08/checkpoints/checkpoint_last.pt\"\n",
    "        self.no_save= False\n",
    "        self.step_checkpoints= False\n",
    "        self.no_log= False\n",
    "        self.log_interval= 100\n",
    "        self.no_visual= False\n",
    "        self.visual_interval= 100\n",
    "        self.no_progress= False\n",
    "        self.draft= False\n",
    "        self.dry_run= False\n",
    "\n",
    "        self.in_channels= 1\n",
    "        self.hidden_size = 64\n",
    "        self.bias= False\n",
    "        self.test_num = 0\n",
    "        self.wtl2 = 0 # l2 loss weighting\n",
    "        # UNET\n",
    "        self.residual = False\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu or cpu\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "utils.setup_experiment(args)\n",
    "utils.init_logging(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build data loaders, a model and an optimizer\n",
    "\n",
    "G,D = models.build_model_gan(args)\n",
    "netG = G.to(device)\n",
    "netD = D.to(device)\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# 2 optimizers\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=args.lr,betas=(0.5, 0.999))\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=args.lr,betas=(0.5, 0.999))\n",
    "\n",
    "logging.info(f\"Built a generator model consisting of {sum(p.numel() for p in netG.parameters()):,} parameters\")\n",
    "logging.info(f\"Built a discriminator model consisting of {sum(p.numel() for p in netD.parameters()):,} parameters\")\n",
    "\n",
    "if args.resume_training and args.restore_file is not None:\n",
    "    state_dict = utils.load_checkpoint_GAN(args, netG, netD, optimizerG, optimizerD)\n",
    "    global_step = state_dict['last_step']\n",
    "    start_epoch = int(state_dict['last_step']/(403200/state_dict['args'].batch_size))+1\n",
    "else:\n",
    "    global_step = -1\n",
    "    start_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_dataset is a function in utils/data/__init__.py\n",
    "train_loaderG, valid_loaderG, _ = data.build_dataset(args.datasetG,\n",
    "                                                   args.n_data, \n",
    "                                                   batch_size=args.batch_size,\n",
    "                                                   min_sep = args.min_sep)\n",
    "train_loaderD, _, _ = data.build_dataset(args.datasetD,\n",
    "                                                   args.n_data, \n",
    "                                                   batch_size=args.batch_size,\n",
    "                                                   min_sep = args.min_sep)\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track moving average of loss values\n",
    "train_meters = {name: utils.RunningAverageMeter(0.98) for name in ([\"train_loss\", \"train_psnr\", \"train_ssim\"])}\n",
    "valid_meters = {name: utils.AverageMeter() for name in ([\"valid_psnr\", \"valid_ssim\"])}\n",
    "writer = SummaryWriter(log_dir=args.experiment_dir) if not args.no_visual else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fake_label = 0.\n",
    "real_label = 1.\n",
    "# fake_label = [0.,0.1]\n",
    "# real_label = [0.9,1.]\n",
    "\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "if args.wtl2 > 0:\n",
    "    use_mse = True\n",
    "else:\n",
    "    use_mse = False\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING\n",
    "for epoch in range(start_epoch, args.num_epochs):\n",
    "    if args.resume_training:\n",
    "        if epoch %10 == 0:\n",
    "            optimizerG.param_groups[0][\"lr\"] /= 2\n",
    "            print('learning rate reduced by factor of 2')\n",
    "\n",
    "    train_bar = utils.ProgressBar(train_loaderG, epoch)\n",
    "    for meter in train_meters.values():\n",
    "        meter.reset()\n",
    "\n",
    "    for batch_id, ((clean, mask),real) in enumerate(zip(train_bar,train_loaderD)):\n",
    "        \n",
    "        ###############################\n",
    "        # First train the discriminator\n",
    "        ###############################\n",
    "        netD.zero_grad()\n",
    "        real_cpu = real.to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,),real_label,device=device)\n",
    "        # Introducing label noise\n",
    "#         label = torch.rand((b_size,),device=device)*(real_label[1]-real_label[0])+real_label[0]\n",
    "\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        ## Train with all-fake batch\n",
    "        # Generate fake signal batch with G\n",
    "        inputs = clean.to(device)\n",
    "        mask_inputs = mask.to(device)\n",
    "        # only use the mask part of the outputs\n",
    "        raw_outputs = netG(inputs,mask_inputs)\n",
    "        fake = (1-mask_inputs)*raw_outputs + mask_inputs*inputs\n",
    "        \n",
    "        label.fill_(fake_label)\n",
    "        # Introducing label noise\n",
    "#         label = torch.rand((b_size,),device=device)*(fake_label[1]-fake_label[0])+fake_label[0]\n",
    "\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "        \n",
    "        ###############################\n",
    "        # Next, train the generator\n",
    "        ###############################\n",
    "        netG.zero_grad() # train() or zero_grad()?\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        \n",
    "        if use_mse:\n",
    "            # Calculate G's loss based on this output\n",
    "            errG_D = criterion(output, label)\n",
    "            # MSE Loss\n",
    "            errG_l2 = F.mse_loss(fake, inputs, reduction=\"sum\") / (inputs.size(0) * 2)\n",
    "            errG =  (1-args.wtl2) * errG_D + args.wtl2 * errG_l2\n",
    "        else:\n",
    "            errG = criterion(output, label)\n",
    "        \n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        \n",
    "\n",
    "        \n",
    "        # Output training stats\n",
    "        if batch_id % 50 == 0:\n",
    "            if use_mse:\n",
    "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f + %.4f = %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, args.num_epochs, batch_id, len(train_loaderG), \\\n",
    "                     errD.item(), errG_D.item(), errG_l2, errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            else: \n",
    "                print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, args.num_epochs, batch_id, len(train_loaderG),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        # TO DO: add a \"fixed input\" to check in on G over time\n",
    "#         if (global_step % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloaderG)-1)):\n",
    "#             with torch.no_grad():\n",
    "#                 fake = netG(fixed_noise).detach().cpu()\n",
    "#             img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        global_step += 1\n",
    "        \n",
    "        # TO DO, only run loss on masked part of output\n",
    "        # TO DO, incorporate MSE loss into GAN?\n",
    "#         loss = F.mse_loss(outputs, inputs, reduction=\"sum\") / (inputs.size(0) * 2)\n",
    "\n",
    "        train_psnr = utils.psnr(fake, inputs)\n",
    "        train_ssim = utils.ssim(fake, inputs)\n",
    "        train_meters[\"train_loss\"].update(errG.item())\n",
    "        train_meters[\"train_psnr\"].update(train_psnr.item())\n",
    "        train_meters[\"train_ssim\"].update(train_ssim.item())\n",
    "        train_bar.log(dict(**train_meters, lr=optimizerG.param_groups[0][\"lr\"]), verbose=True)\n",
    "\n",
    "        if writer is not None and global_step % args.log_interval == 0:\n",
    "            writer.add_scalar(\"lr\", optimizerG.param_groups[0][\"lr\"], global_step)\n",
    "            writer.add_scalar(\"loss/train\", errG.item(), global_step)\n",
    "            writer.add_scalar(\"psnr/train\", train_psnr.item(), global_step)\n",
    "            writer.add_scalar(\"ssim/train\", train_ssim.item(), global_step)\n",
    "            gradients = torch.cat([p.grad.view(-1) for p in netG.parameters() if p.grad is not None], dim=0)\n",
    "            writer.add_histogram(\"gradients\", gradients, global_step)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    if epoch % args.valid_interval == 0:\n",
    "        netG.eval()\n",
    "        for meter in valid_meters.values():\n",
    "            meter.reset()\n",
    "\n",
    "        valid_bar = utils.ProgressBar(valid_loaderG)\n",
    "        \n",
    "        for sample_id, (clean, mask) in enumerate(valid_bar):\n",
    "            with torch.no_grad():\n",
    "                inputs = clean.to(device)\n",
    "                mask_inputs = mask.to(device)\n",
    "                # only use the mask part of the outputs\n",
    "                raw_output = netG(inputs,mask_inputs)\n",
    "                output = (1-mask_inputs)*raw_output + mask_inputs*inputs\n",
    "                valid_psnr = utils.psnr(inputs, output)\n",
    "                valid_meters[\"valid_psnr\"].update(valid_psnr.item())\n",
    "                valid_ssim = utils.ssim(inputs, output)\n",
    "                valid_meters[\"valid_ssim\"].update(valid_ssim.item())\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"psnr/valid\", valid_meters['valid_psnr'].avg, global_step)\n",
    "            writer.add_scalar(\"ssim/valid\", valid_meters['valid_ssim'].avg, global_step)\n",
    "            sys.stdout.flush()\n",
    "            utils.save_checkpoint_GAN(args, global_step, netG, netD, optimizerG, optimizerD, score=valid_meters[\"valid_psnr\"].avg, mode=\"max\")\n",
    "\n",
    "        logging.info(train_bar.print(dict(**train_meters, **valid_meters, lr=optimizerG.param_groups[0][\"lr\"])))\n",
    "        # Save the loss curve plot\n",
    "        utils.save_losses_curve(G_losses,D_losses)\n",
    "# logging.info(f\"Done training! Best PSNR {utils.save_checkpoint_GAN.best_score:.3f} obtained after step {utils.save_checkpoint.best_step}.\")\n",
    "logging.info(f\"Done training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_losses_curve(G_losses,D_losses,filename='Loss_curve'):\n",
    "    k=0\n",
    "    plt.figure(figsize=[15,8])\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.title(\"Generator Loss\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Total batch number\")\n",
    "    plt.plot(G_losses[k:],'bx')\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plt.title(\"Discriminator Loss\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xlabel(\"Total batch number\")\n",
    "    plt.plot(D_losses[k:],'rx')\n",
    "    plt.savefig(os.path.join(args.checkpoint_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, (b,c) in enumerate(train_bar):\n",
    "    print(a)\n",
    "    print(b.shape)\n",
    "    print(c.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Best PSNR 28.560\n",
    "def mask_idx_f(mask):\n",
    "    mask_start = int(np.argmin(mask[0]))\n",
    "    mask_length = int((1-mask[0]).sum())\n",
    "    mask_idx = range(mask_start,mask_start+mask_length)\n",
    "     # No mask indices\n",
    "    before = np.arange(mask.shape[2])[:mask_start]\n",
    "    after = np.arange(mask.shape[2])[mask_start+mask_length:]\n",
    "    no_mask_idx = np.append(before,after)\n",
    "    return mask_idx,before, after, mask_length, mask_start\n",
    "\n",
    "def print_one(loader,model1,model2):\n",
    "    np.random.seed()\n",
    "    clean,mask = next(iter(loader))\n",
    "    outputs1 = model1(clean.to(device),mask.to(device)).cpu()\n",
    "    outputs2 = model2(clean.to(device),mask.to(device)).cpu()\n",
    "    \n",
    "    mask_idx,before_mask,after_mask,mask_length, mask_start = mask_idx_f(mask)\n",
    "\n",
    "    out1 = outputs1[0] * (1-mask[0]) + clean[0]*mask[0]\n",
    "    out2 = outputs2[0] * (1-mask[0]) + clean[0]*mask[0]\n",
    "\n",
    "    print(\"Mask Length: {}\\tMask Start: {}\".format(mask_length,mask_start))\n",
    "    \n",
    "    plt.figure(figsize=[15,14])\n",
    "    plt.subplot(4,1,1)\n",
    "    plt.plot(clean[0,0,:],'xb')\n",
    "    plt.plot(mask_idx,np.zeros(len(mask_idx)),'--k')\n",
    "    plt.plot(mask_idx,np.ones(len(mask_idx)),'--k')\n",
    "    plt.title(\"True signal\")\n",
    "\n",
    "    plt.subplot(4,1,2)\n",
    "    masked = clean[0]*mask[0]\n",
    "    masked_plot = masked[:mask_start,]\n",
    "    plt.plot(before_mask,masked[0,before_mask],'xb')\n",
    "    plt.plot(after_mask,masked[0,after_mask],'xb')\n",
    "    plt.plot(mask_idx,np.zeros(len(mask_idx)),'--k')\n",
    "    plt.plot(mask_idx,np.ones(len(mask_idx)),'--k')\n",
    "    plt.title(\"Masked signal\")\n",
    "\n",
    "    plt.subplot(4,1,3)\n",
    "    plt.plot(out1[0,:].detach(),'xb')\n",
    "    plt.plot(mask_idx,np.zeros(len(mask_idx)),'--k')\n",
    "    plt.plot(mask_idx,np.ones(len(mask_idx)),'--k')\n",
    "    plt.title(\"GAN denoised signal\")\n",
    "    print(outputs1[0,:].detach())\n",
    "    \n",
    "    plt.subplot(4,1,4)\n",
    "    plt.plot(out2[0,:].detach(),'xb')\n",
    "    plt.plot(mask_idx,np.zeros(len(mask_idx)),'--k')\n",
    "    plt.plot(mask_idx,np.ones(len(mask_idx)),'--k')\n",
    "    plt.title(\"MSE denoised signal\")\n",
    "#     return out1,out2, clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MSE model\n",
    "mse = models.build_model(args)\n",
    "netMSE = mse.to(device)\n",
    "\n",
    "MODEL_PATH = \"models/trained/unet1d_partialconv_10kdata_30epoch_3minsep_08_14_20.pth\"\n",
    "netMSE.load_state_dict(torch.load(MODEL_PATH))\n",
    "netMSE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loader is shuffled and allows test_num to force a certain mask shape\n",
    "_, _, test_loader = data.build_dataset(args.datasetG,\n",
    "                                                   1000, \n",
    "                                                   batch_size=1,\n",
    "                                                   fix_datapoints=False,\n",
    "                                                   min_sep = args.min_sep,\n",
    "                                                   test_num = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_one(test_loader,netG,netMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_one(test_loader,netG,netMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_one(test_loader,netG,netMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_one(test_loader,netG,netMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_one(test_loader,netG,netMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_one(test_loader,netG,netMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(D_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the model\n",
    "MODEL_PATHG = \"models/trained/GAN_unet_generator_100kdata_300epoch_08_20_20.pth\"\n",
    "MODEL_PATHD = \"models/trained/GAN_discriminator_100kdata_300epoch_08_20_20.pth\"\n",
    "torch.save(netG.state_dict(), MODEL_PATHG)\n",
    "torch.save(netD.state_dict(), MODEL_PATHD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
