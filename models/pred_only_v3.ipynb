{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/michael/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/michael/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/michael/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/michael/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/michael/python-virtual-environments/pointscnn/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michael/lidar/D-FCN\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import socket\n",
    "import importlib\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "#import h5pyprovider\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import matplotlib.pylab as plt\n",
    "# os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']=\"1\"\n",
    "\n",
    "import sys\n",
    "BASE_DIR = os.path.abspath('')\n",
    "print(BASE_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, 'models'))\n",
    "sys.path.append(BASE_DIR) # model\n",
    "sys.path.append(os.path.join(BASE_DIR, 'tf_utils'))\n",
    "import provider\n",
    "import tf_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argparses\n",
    "EPOCH_CNT = 0\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "NUM_POINT = 2048\n",
    "MAX_EPOCH = 200\n",
    "BASE_LEARNING_RATE = .01\n",
    "# GPU_INDEX = 7\n",
    "GPU_INDEX = 1\n",
    "MOMENTUM = 0.9\n",
    "OPTIMIZER = 'adam'\n",
    "DECAY_STEP = 20000\n",
    "DECAY_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR /home/michael/lidar/D-FCN/tf_utils\n"
     ]
    }
   ],
   "source": [
    "MODEL = importlib.import_module('DFCN_pointnet2_group2') # import network module\n",
    "MODEL_FILE = os.path.join(BASE_DIR, 'DFCN_pointnet2_group2'+'.py')\n",
    "LOG_DIR = 'log_wen_v16_sample8192_group2_lw14_F1_noheight'\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "os.system('cp models/DFCN_pointnet2.py %s' % (LOG_DIR)) # bkp of model def\n",
    "os.system('cp tf_utils/DFCN_util_xy2.py %s' % (LOG_DIR)) # bkp of model def\n",
    "# os.system('cp train_pointsift_lx_npsplit_V16.ipynb %s' % (LOG_DIR)) # bkp of train procedure\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "# LOG_FOUT.write(str(FLAGS)+'\\n')\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "HOSTNAME = socket.gethostname()\n",
    "\n",
    "NUM_CLASSES = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n",
    "\n",
    "def get_learning_rate(batch):\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "                        BASE_LEARNING_RATE,  # Base learning rate.\n",
    "                        batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                        DECAY_STEP,          # Decay step.\n",
    "                        DECAY_RATE,          # Decay rate.\n",
    "                        staircase=True)\n",
    "    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!!\n",
    "    return learning_rate        \n",
    "\n",
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay\n",
    "\n",
    "def Acc_from_confusions(confusions):\n",
    "    \n",
    "    TP = np.diagonal(confusions, axis1=-2, axis2=-1)\n",
    "    TP_plus_FN = np.sum(confusions, axis=-1)\n",
    "    TP_plus_FP = np.sum(confusions, axis=-2)\n",
    "    \n",
    "    mAcc = np.sum(TP)/np.sum(confusions)\n",
    "    \n",
    "    precision = TP / (TP_plus_FP + 1e-6)\n",
    "    recall = TP / (TP_plus_FN+ 1e-6)\n",
    "    fscore = 2*(precision * recall)/(precision + recall + 1e-6)\n",
    "    \n",
    "    ave_F1 = np.mean(fscore)\n",
    "    \n",
    "    s = 'Overall accuracy：{:5.2f}  Average F1 score：{:5.2f} \\n'.format(100 * mAcc, 100 * ave_F1)\n",
    "    s += log_acc(precision)\n",
    "    s += log_acc(recall)\n",
    "    s += log_acc(fscore)\n",
    "    \n",
    "    log_string(s)\n",
    "    \n",
    "    return mAcc, ave_F1\n",
    "    \n",
    "\n",
    "def log_acc(acc_list):\n",
    "    s = \"\"\n",
    "    for acc in acc_list:\n",
    "        s += '{:5.2f} '.format(100 * acc)\n",
    "    s += '\\n'\n",
    "    return s\n",
    "\n",
    "def pc_normalize(pc):\n",
    "    l = pc.shape[0]\n",
    "    centroid = np.mean(pc, axis=0)\n",
    "    pc = pc - centroid\n",
    "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
    "    pc = pc / m\n",
    "    return pc\n",
    "    \n",
    "def drawPlot(x,y,name):\n",
    "    plt.rcParams['savefig.dpi'] = 300 \n",
    "    plt.plot(np.arange(0,len(x)),x,'k-',alpha=1,label='Train max: '+str(round(max(x),3))+', min: '+str(round(min(x),3)))\n",
    "    plt.plot(np.arange(0,len(y)),y,'r-',alpha=1,label='Test max: '+str(round(max(y),3))+', min: '+str(round(min(y),3)))\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch',fontsize=9)\n",
    "    plt.ylabel(name+' value',fontsize=9)\n",
    "    plt.savefig(LOG_DIR+\"/\"+name+\".png\",bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def drawF1Plot(x,name):\n",
    "    plt.rcParams['savefig.dpi'] = 300 \n",
    "    plt.plot(np.arange(0,len(x)),x,'k-',alpha=1,label='Train max: '+str(round(max(x),3))+', min: '+str(round(min(x),3)))\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch',fontsize=9)\n",
    "    plt.ylabel(name+' value',fontsize=9)\n",
    "    plt.savefig(LOG_DIR+\"/\"+name+\".png\",bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def placeholder_inputs(batch_size, num_point):\n",
    "    pointclouds_pl = tf.placeholder(tf.float32, shape=(None, None, 3))\n",
    "    labels_pl = tf.placeholder(tf.int32, shape=(None, None))\n",
    "    smpws_pl = tf.placeholder(tf.float32, shape=(None, None))\n",
    "    return pointclouds_pl, labels_pl, smpws_pl\n",
    "\n",
    "def pc_normalize_min_max(data):\n",
    "    mindata = np.min(data[:,:3], axis=0)\n",
    "    maxdata = np.max(data[:,:3], axis=0)\n",
    "    return 2*(data[:,:3] - mindata)/(maxdata - mindata)\n",
    "\n",
    "def pc_normalize_min(data):\n",
    "    mindata = np.min(data[:,:3], axis=0)\n",
    "    \n",
    "    return (data[:,:3] - mindata)\n",
    "\n",
    "def get_batch(dataset, index, npoints = NUM_POINT):\n",
    "  \n",
    "    if(dataset =='train'):\n",
    "        cub_l = 30.0\n",
    "        cub_w = 30.0\n",
    "        cub_h = 100.0\n",
    "        # Shift xyz to each start at 0. \n",
    "        point_set =  trainSet[:,:3] - np.min(trainSet[:,:3], axis=0)\n",
    "        semantic_seg = trainSet[:,4].astype(np.int32)\n",
    "        coordmax = np.max(point_set,axis=0)\n",
    "        coordmin = np.min(point_set,axis=0)\n",
    "        smpmin = np.maximum(coordmax-[cub_l,cub_w,cub_h], coordmin)\n",
    "        smpmin[2] = coordmin[2]\n",
    "        smpsz = np.minimum(coordmax-smpmin,[cub_l,cub_w,cub_h])\n",
    "        smpsz[2] = coordmax[2]-coordmin[2]\n",
    "        isvalid = False\n",
    "        for i in range(10):\n",
    "            # randomly select one record from point_set, set it as your center\n",
    "            curcenter = point_set[np.random.choice(len(semantic_seg),1)[0],:]\n",
    "            # Draw the cube around the curcenter\n",
    "            curmin = curcenter-[cub_l/2,cub_w/2,cub_h/2]\n",
    "            curmax = curcenter+[cub_l/2,cub_w/2,cub_h/2]\n",
    "            curmin[2] = coordmin[2]\n",
    "            curmax[2] = coordmax[2]\n",
    "            # Filter point_set for points within the cube (==3 requires all 3 axes to be true)\n",
    "            curchoice = np.sum((point_set>=(curmin-0.0))*(point_set<(curmax+0.0)),axis=1)==3\n",
    "            cur_point_set = point_set[curchoice,:]\n",
    "            cur_semantic_seg = semantic_seg[curchoice]\n",
    "            cur_feat_set = trainFeats[curchoice,:]\n",
    "    #         if len(cur_semantic_seg)<npoints:\n",
    "            if len(cur_semantic_seg)==0:\n",
    "                continue\n",
    "            mask = np.sum((cur_point_set>=(curmin-0.0))*(cur_point_set<(curmax+0.0)),axis=1)==3\n",
    "            vidx = np.ceil((cur_point_set[mask,:2]-curmin[:2])/(curmax[:2]-curmin[:2])*[31.0,31.0])\n",
    "            vidx = np.unique(vidx[:,0]*31.0+vidx[:,1])\n",
    "            isvalid = np.sum(cur_semantic_seg>-1)/1.0/len(cur_semantic_seg)>=0.7 and len(vidx)/31.0/31.0>=0.3\n",
    "#             print('isvalid', isvalid,len(vidx)/31.0/31.0,np.sum(cur_semantic_seg>-1),len(cur_semantic_seg))\n",
    "            if isvalid:\n",
    "                break\n",
    "        # Select the points (with replacement) that will be used in the surrounding cloud\n",
    "        choice = np.random.choice(len(cur_semantic_seg), npoints, replace=True)\n",
    "        point_set = cur_point_set[choice,:]\n",
    "        feature_set = cur_feat_set[choice,:]\n",
    "        semantic_seg = cur_semantic_seg[choice]\n",
    "        mask = mask[choice]\n",
    "        sample_weight = labelweights[semantic_seg]\n",
    "        sample_weight *= mask\n",
    "        return point_set, semantic_seg, sample_weight,feature_set\n",
    "    \n",
    "    if(dataset =='test'):\n",
    "\n",
    "        cur_point_set = test_xyz[index]\n",
    "        cur_semantic_seg = test_label[index].astype(np.int32)\n",
    "        feature_set = test_feats[index]\n",
    "\n",
    "        point_set = pc_normalize_min(cur_point_set)\n",
    "        semantic_seg = cur_semantic_seg # N\n",
    "        sample_weight = labelweights_t[semantic_seg]\n",
    "    \n",
    "        point_sets = np.expand_dims(point_set,0) # 1xNx3\n",
    "        feature_set = np.expand_dims(feature_set,0) # 1xNx3\n",
    "        semantic_segs = np.expand_dims(semantic_seg,0)  # 1xN\n",
    "        sample_weights = np.expand_dims(sample_weight,0)  # 1xN\n",
    "        return point_sets, semantic_segs, sample_weights,feature_set\n",
    "    \n",
    "def get_batch_wdp(dataset, batch_idx):\n",
    "    bsize = BATCH_SIZE\n",
    "    batch_data = np.zeros((bsize, NUM_POINT, 3))\n",
    "    batch_feats = np.zeros((bsize, NUM_POINT, 1))\n",
    "    batch_label = np.zeros((bsize, NUM_POINT), dtype=np.int32)\n",
    "    batch_smpw = np.zeros((bsize, NUM_POINT), dtype=np.float32)\n",
    "    for i in range(bsize):\n",
    "        ps,seg,smpw,feat = get_batch(dataset,index=0)\n",
    "        print(\"In get_batch_wdp, ps.shape\",ps.shape)\n",
    "        ps = pc_normalize_min(ps)\n",
    "        batch_data[i,...] = ps\n",
    "        batch_label[i,:] = seg\n",
    "        batch_smpw[i,:] = smpw\n",
    "        batch_feats[i,:] = feat\n",
    "\n",
    "        dropout_ratio = np.random.random()*0.875 # 0-0.875\n",
    "        drop_idx = np.where(np.random.random((ps.shape[0]))<=dropout_ratio)[0]\n",
    "        batch_data[i,drop_idx,:] = batch_data[i,0,:]\n",
    "        batch_label[i,drop_idx] = batch_label[i,0]\n",
    "        batch_smpw[i,drop_idx] *= 0\n",
    "        \n",
    "    return batch_data, batch_label, batch_smpw, batch_feats   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In get_batch_wdp, ps.shape (2048, 3)\n",
      "In get_batch_wdp, ps.shape (2048, 3)\n",
      "In get_batch_wdp, ps.shape (2048, 3)\n",
      "In get_batch_wdp, ps.shape (2048, 3)\n",
      "In get_batch_wdp, ps.shape (2048, 3)\n",
      "In get_batch_wdp, ps.shape (2048, 3)\n"
     ]
    }
   ],
   "source": [
    "batch_data, batch_label, batch_smpw, batch_feats = get_batch_wdp('train', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.875     , 18.5       ,  1.76998901])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSet[0]\n",
    "batch_data[0][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.967452  2.0217078 1.9802364 2.9339767 2.874856  2.1228907 2.7627099\n",
      " 2.6275027 2.1882594]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# train_f = open('Data/train_merge_min_norm_fea.pickle', 'rb')\n",
    "# train_xyz, train_label, train_feats = pickle.load(train_f, encoding='bytes')\n",
    "# train_xyz, train_label, train_feats = pickle.load(train_f)\n",
    "# train_f.close()\n",
    "\n",
    "#test_f = open('Data/test_merge_min_norm_fea_paper_height.pickle', 'rb')\n",
    "#test_xyz, test_label, test_feats = pickle.load(test_f, encoding='bytes')\n",
    "#test_feats = [tt[:,1:2] for tt in test_feats] #reflectance\n",
    "#test_f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "testSet = np.loadtxt('Data/train_height.pts',skiprows=1)\n",
    "\n",
    "label_w = testSet[:,4].astype('uint8')\n",
    "testSet[:,3] = testSet[:,3]/testSet[:,3].max() #height above ground\n",
    "testSet[:,5] = testSet[:,5]/testSet[:,5].max() #reflectance\n",
    "\n",
    "# trainFeats = trainSet[:,[3,5]] #use reflectance and height above ground\n",
    "testFeats = testSet[:,5:6] #only use reflectance\n",
    "\n",
    "\n",
    "\n",
    "NUM_CLASSES = 9\n",
    "label_values = range(NUM_CLASSES)\n",
    "\n",
    "trainSet = np.loadtxt('Data/train_height.pts',skiprows=1)\n",
    "\n",
    "label_w = trainSet[:,4].astype('uint8')\n",
    "trainSet[:,3] = trainSet[:,3]/trainSet[:,3].max() #height above ground\n",
    "trainSet[:,5] = trainSet[:,5]/trainSet[:,5].max() #reflectance\n",
    "\n",
    "# trainFeats = trainSet[:,[3,5]] #use reflectance and height above ground\n",
    "trainFeats = trainSet[:,5:6] #only use reflectance\n",
    "\n",
    "labelweights = np.zeros(9)\n",
    "tmp,_ = np.histogram(label_w,range(10))\n",
    "labelweights = tmp\n",
    "labelweights = labelweights.astype(np.float32)\n",
    "labelweights = labelweights/np.sum(labelweights)\n",
    "labelweights = 1/np.log(1.4+labelweights)\n",
    "print(labelweights)\n",
    "\n",
    "labelweights_t = np.ones(9)\n",
    "print(labelweights_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(sess, ops, train_writer):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = True\n",
    "    \n",
    "    log_string('----')\n",
    "    \n",
    "   # Shuffle train samples\n",
    "    train_idxs = np.arange(0, len(train_xyz))\n",
    "    np.random.shuffle(train_idxs)\n",
    "    num_batches = len(train_xyz)//BATCH_SIZE\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        \n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "        \n",
    "        batch_data, batch_label, batch_smpw, batch_feats = get_batch_wdp('train', batch_idx)\n",
    "        \n",
    "        if batch_idx % (num_batches/2) == 0:\n",
    "            print('Current batch/total batch num: %d/%d'%(batch_idx,num_batches))\n",
    "        \n",
    "        aug_data = provider.rotate_point_cloud_z(batch_data)\n",
    "        \n",
    "        feed_dict = {ops['pointclouds_pl']: aug_data,\n",
    "                     ops['feature_pl']: batch_feats,\n",
    "                     ops['labels_pl']: batch_label,\n",
    "                     ops['smpws_pl']: batch_smpw,\n",
    "                     ops['is_training_pl']: is_training,}\n",
    "        summary, step, _, loss_val, pred_val, lr_val = sess.run([ops['merged'], ops['step'], ops['train_op'], ops['loss'], ops['pred'], ops['learnrate']],\n",
    "                                         feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, step)\n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum(pred_val == batch_label)\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE*NUM_POINT)\n",
    "        loss_sum += loss_val\n",
    "        \n",
    "    log_string('learn rate: %f' % (lr_val))\n",
    "    log_string('mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    log_string('accuracy: %f' % (total_correct / float(total_seen)))\n",
    "    \n",
    "    mloss = loss_sum / float(num_batches)\n",
    "    macc = total_correct / float(total_seen)\n",
    "    return mloss, macc\n",
    "\n",
    "def eval_one_epoch_whole_scene(sess, ops):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    log_string('----')\n",
    "    \n",
    "    test_idxs = np.arange(0, len(test_xyz))\n",
    "    # Shuffle train samples\n",
    "    np.random.shuffle(test_idxs)\n",
    "    num_batches = len(test_xyz)//BATCH_SIZE\n",
    "    \n",
    "    #TEST_BATCH_SIZE = 1\n",
    "    #num_batches = len(test_xyz)\n",
    "    \n",
    "    Confs = []\n",
    "    #\n",
    "    pred_list = []\n",
    "    \n",
    "    is_continue_batch = False\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        \n",
    "#         batch_data_t, batch_label_t, batch_smpw_t, batch_feats_t = get_batch('train', batch_idx)\n",
    "        \n",
    "#         print('Current start end /total batch num: %d %d/%d'%(start_idx, end_idx, num_batches))\n",
    "        \n",
    "        ##aug_data = batch_data\n",
    "#        \n",
    "\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = (batch_idx+1) * BATCH_SIZE\n",
    "        \n",
    "        batch_data, batch_label, batch_smpw, batch_feats = get_batch_wdp('test', batch_idx)\n",
    "        \n",
    "        if batch_idx % (num_batches/2) == 0:\n",
    "            print('Current batch/total batch num: %d/%d'%(batch_idx,num_batches))\n",
    "\n",
    "            \n",
    "        aug_data = provider.rotate_point_cloud_z(batch_data)\n",
    "        \n",
    "        feed_dict = {ops['pointclouds_pl']: aug_data,\n",
    "                     ops['feature_pl']: batch_feats,\n",
    "                     ops['labels_pl']: batch_label,\n",
    "                     ops['smpws_pl']: batch_smpw,\n",
    "                     ops['is_training_pl']: is_training}\n",
    "#         print(\"feed_dict: \",feed_dict)\n",
    "        #summary, step, loss_val, pred_val, lr_val = sess.run([ops['merged'], ops['step'], ops['loss'], ops['pred'], ops['learnrate']],\n",
    "                                      #feed_dict=feed_dict)\n",
    "        \n",
    "        summary, step, _, loss_val, pred_val, lr_val = sess.run([ops['merged'], ops['step'], ops['train_op'], ops['loss'], ops['pred'], ops['learnrate']],\n",
    "                                         feed_dict=feed_dict)\n",
    "        \n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        print(\"pred_val shape: \",pred_val.shape)\n",
    "        correct = np.sum(pred_val == batch_label)\n",
    "        ##original\n",
    "        #pred_list.append([pred_val,correct])\n",
    "        ##add feed_dict with pred_val\n",
    "        pred_list.append([aug_data,pred_val])\n",
    "        #pred_list.append([batch_data_t,pred_val])\n",
    "                \n",
    "        total_correct += correct\n",
    "        #total_seen += batch_data.shape[1]\n",
    "        total_seen += (BATCH_SIZE*NUM_POINT)\n",
    "        loss_sum += loss_val\n",
    "        \n",
    "       \n",
    "        #NUM_POINT_fact = batch_data.shape[1]\n",
    "        NUM_POINT_fact = (BATCH_SIZE*NUM_POINT)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            for j in range(NUM_POINT):\n",
    "                l = batch_label[i, j]\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_val[i, j] == l)\n",
    "                \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        Confs += [confusion_matrix(batch_label.flatten(), pred_val.flatten(), label_values)]\n",
    "    \n",
    "    #end of for loop\n",
    "    C = np.sum(np.stack(Confs), axis=0).astype(np.float32)\n",
    "    oa, avgF1 = Acc_from_confusions(C)\n",
    "   \n",
    "    log_string('learn rate: %f' % (lr_val))\n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "    log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
    "    \n",
    "    mloss = loss_sum / float(num_batches)\n",
    "    macc = total_correct / float(total_seen)\n",
    "    return oa, avgF1,pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def eval_one_epoch_whole_scene(sess, ops):\n",
    "    \"\"\" ops: dict mapping from string to tf ops \"\"\"\n",
    "    is_training = False\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    log_string('----')\n",
    "    \n",
    "    test_idxs = np.arange(0, len(test_xyz))\n",
    "    \n",
    "    TEST_BATCH_SIZE = 1\n",
    "    num_batches = len(test_xyz)\n",
    "    \n",
    "    Confs = []\n",
    "    \n",
    "    \n",
    "    is_continue_batch = False\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        \n",
    "        batch_data, batch_label, batch_smpw, batch_feats = get_batch('test', batch_idx)\n",
    "        \n",
    "#         print('Current start end /total batch num: %d %d/%d'%(start_idx, end_idx, num_batches))\n",
    "        \n",
    "        aug_data = batch_data\n",
    "        \n",
    "#         aug_data = provider.rotate_point_cloud_z(batch_data)\n",
    "        \n",
    "        feed_dict = {ops['pointclouds_pl']: aug_data,\n",
    "                     ops['feature_pl']: batch_feats,\n",
    "                     ops['labels_pl']: batch_label,\n",
    "                     ops['smpws_pl']: batch_smpw,\n",
    "                     ops['is_training_pl']: is_training}\n",
    "        print(\"feed_dict: \",feed_dict)\n",
    "        summary, step, loss_val, pred_val, lr_val = sess.run([ops['merged'], ops['step'], ops['loss'], ops['pred'], ops['learnrate']],\n",
    "                                      feed_dict=feed_dict)\n",
    "        \n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum(pred_val == batch_label)\n",
    "        total_correct += correct\n",
    "        total_seen += batch_data.shape[1]\n",
    "        loss_sum += loss_val\n",
    "        \n",
    "        NUM_POINT_fact = batch_data.shape[1]\n",
    "        for i in range(TEST_BATCH_SIZE):\n",
    "            for j in range(NUM_POINT_fact):\n",
    "                l = batch_label[i, j]\n",
    "                total_seen_class[l] += 1\n",
    "                total_correct_class[l] += (pred_val[i, j] == l)\n",
    "                \n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        Confs += [confusion_matrix(batch_label.flatten(), pred_val.flatten(), label_values)]\n",
    "        \n",
    "    C = np.sum(np.stack(Confs), axis=0).astype(np.float32)\n",
    "    oa, avgF1 = Acc_from_confusions(C)\n",
    "    \n",
    "    log_string('learn rate: %f' % (lr_val))\n",
    "    log_string('eval mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    log_string('eval accuracy: %f'% (total_correct / float(total_seen)))\n",
    "    log_string('eval avg class acc: %f' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n",
    "    \n",
    "    mloss = loss_sum / float(num_batches)\n",
    "    macc = total_correct / float(total_seen)\n",
    "    return oa, avgF1, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "test_xyz,test_label,test_feats = [],[],[]\n",
    "num_test_samples = 200\n",
    "for i in range(num_test_samples):\n",
    "    test_point_set, test_semantic_seg, test_sample_weight, test_feature_set = get_batch('train',0,\n",
    "                                                                                            npoints=NUM_POINT)\n",
    "    test_xyz.append(test_point_set)\n",
    "    test_label.append(test_semantic_seg)\n",
    "    #test_weights.append(test_sample_weight)##\n",
    "    test_feats.append(test_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.333333333333336"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Just clears out previous sessions\n",
    "tf.reset_default_graph()\n",
    "K.clear_session()\n",
    "\n",
    "# Run the session, add the parameters to the graph, testing is run in the eval_one_epoch_whole_scene line\n",
    "with tf.Session() as sess:\n",
    "    pointclouds_pl, labels_pl, smpws_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT)\n",
    "    feature_pl = tf.placeholder(tf.float32, shape=(None, None, 1))\n",
    "    is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "    # Note the global_step=batch parameter to minimize. \n",
    "    # That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "    batch = tf.Variable(0)\n",
    "    bn_decay = get_bn_decay(batch)\n",
    "    tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "    # Get model and loss \n",
    "    pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, NUM_CLASSES, bn_decay=bn_decay, feature=feature_pl)\n",
    "    loss = MODEL.get_loss(pred, labels_pl, smpws_pl)\n",
    "\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    correct = tf.equal(tf.argmax(pred, 2), tf.to_int64(labels_pl))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE*NUM_POINT)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Get training operator\n",
    "    learning_rate = get_learning_rate(batch)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    if OPTIMIZER == 'momentum':\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n",
    "    elif OPTIMIZER == 'adam':\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=batch)\n",
    "    \n",
    "    # Add summary writers\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    ops = {'pointclouds_pl': pointclouds_pl,\n",
    "       'labels_pl': labels_pl,\n",
    "       'feature_pl': feature_pl,\n",
    "       'smpws_pl': smpws_pl,\n",
    "       'is_training_pl': is_training_pl,\n",
    "       'pred': pred,\n",
    "       'loss': loss,\n",
    "       'train_op': train_op,\n",
    "       'merged': merged,\n",
    "       'step': batch,\n",
    "       'learnrate': learning_rate}\n",
    "    \n",
    "    saver = tf.train.import_meta_graph(os.path.join(LOG_DIR, 'model.ckpt.meta'))\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(LOG_DIR))\n",
    "\n",
    "    # This runs the testing\n",
    "    test_acc, test_F1, pred_list = eval_one_epoch_whole_scene(sess, ops)\n",
    "#     print(\"pred_list: \",pred_list)\n",
    "\n",
    "# Not sure we need this, but clears the session\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    \n",
    "    traindata_and_label=np.column_stack((TEST_DATASET.scene_points_list, TEST_DATASET.semantic_labels_list))#np.column_stack将两个矩阵进行组合连接\n",
    "    filename=''.join([\"data/test_dataset/test_data_and_label_\",str(1),'.txt'])\n",
    "    np.savetxt(filename, testdata_and_label,fmt=\"%.8f,%.8f,%.8f,%d\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (test_acc, test_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = pred_list\n",
    "pt = np.empty([len(pred_list),len(pred_list[0][0]),len(pred_list[0][0][0]),4])\n",
    "for batch in range(0,len(pred_list)):\n",
    "    for i in range(0,len(pred_list[batch][0])):\n",
    "        pt[batch][i]=np.column_stack((pred_list[batch][0][i],pred_list[batch][1][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#//X Y Z Classification\n",
    "#output_path = /home/az/Desktop/test.txt\n",
    "f = open(\"/home/az/Desktop/test_output3.txt\", \"w\")\n",
    "#!!!\n",
    "f.write(\"//X Y Z Classification\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l1 in pt:\n",
    "    for l2 in l1:\n",
    "        for l3 in l2:\n",
    "            #s = \", \".join(map(str, l3))\n",
    "            s = \" \".join(map(str, l3))\n",
    "            for item in s:\n",
    "                f.write(item)\n",
    "            f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
